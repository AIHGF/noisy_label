I1006 11:29:07.705499 26389 caffe.cpp:157] Use GPU with device ID 0
I1006 11:29:07.706641 26389 caffe.cpp:165] Starting Optimization
I1006 11:29:07.706745 26389 solver.cpp:37] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.001
display: 100
max_iter: 6000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.004
stepsize: 4000
snapshot: 6000
snapshot_prefix: "external/exp/models/cifar10_noisy_gt_ft_clean"
solver_mode: CPU
net: "models/cifar10_noisy_gt_ft_clean_trainval.prototxt"
test_initialization: true
average_loss: 100
I1006 11:29:07.706784 26389 solver.cpp:75] Creating training net from net file: models/cifar10_noisy_gt_ft_clean_trainval.prototxt
I1006 11:29:07.707125 26389 net.cpp:307] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1006 11:29:07.707139 26389 net.cpp:307] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1006 11:29:07.707273 26389 net.cpp:46] Initializing net from parameters: 
name: "cifar10_noisy_gt"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "external/exp/db/cifar10/cifar10_mean.binaryproto"
  }
  data_param {
    source: "external/exp/db/cifar10/mixed_train"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1006 11:29:07.707329 26389 layer_factory.hpp:74] Creating layer cifar
I1006 11:29:07.707340 26389 net.cpp:110] Creating Layer cifar
I1006 11:29:07.707345 26389 net.cpp:388] cifar -> data
I1006 11:29:07.707376 26389 net.cpp:388] cifar -> label
I1006 11:29:07.707387 26389 net.cpp:140] Setting up cifar
I1006 11:29:07.707393 26389 data_transformer.cpp:22] Loading mean file from: external/exp/db/cifar10/cifar10_mean.binaryproto
I1006 11:29:07.707518 26389 db_lmdb.cpp:22] Opened lmdb external/exp/db/cifar10/mixed_train
I1006 11:29:07.707553 26389 data_layer.cpp:55] Skipping first 0 data points.
I1006 11:29:07.707569 26389 data_layer.cpp:100] output data size: 100,3,32,32
I1006 11:29:07.707638 26389 net.cpp:147] Top shape: 100 3 32 32 (307200)
I1006 11:29:07.707644 26389 net.cpp:147] Top shape: 100 (100)
I1006 11:29:07.707661 26389 layer_factory.hpp:74] Creating layer conv1
I1006 11:29:07.707670 26389 net.cpp:110] Creating Layer conv1
I1006 11:29:07.707674 26389 net.cpp:430] conv1 <- data
I1006 11:29:07.707684 26389 net.cpp:388] conv1 -> conv1
I1006 11:29:07.707705 26389 net.cpp:140] Setting up conv1
I1006 11:29:07.783834 26389 net.cpp:147] Top shape: 100 32 32 32 (3276800)
I1006 11:29:07.783874 26389 layer_factory.hpp:74] Creating layer pool1
I1006 11:29:07.783888 26389 net.cpp:110] Creating Layer pool1
I1006 11:29:07.783892 26389 net.cpp:430] pool1 <- conv1
I1006 11:29:07.783900 26389 net.cpp:388] pool1 -> pool1
I1006 11:29:07.783920 26389 net.cpp:140] Setting up pool1
I1006 11:29:07.784054 26389 net.cpp:147] Top shape: 100 32 16 16 (819200)
I1006 11:29:07.784062 26389 layer_factory.hpp:74] Creating layer relu1
I1006 11:29:07.784067 26389 net.cpp:110] Creating Layer relu1
I1006 11:29:07.784070 26389 net.cpp:430] relu1 <- pool1
I1006 11:29:07.784073 26389 net.cpp:377] relu1 -> pool1 (in-place)
I1006 11:29:07.784080 26389 net.cpp:140] Setting up relu1
I1006 11:29:07.784296 26389 net.cpp:147] Top shape: 100 32 16 16 (819200)
I1006 11:29:07.784303 26389 layer_factory.hpp:74] Creating layer conv2
I1006 11:29:07.784313 26389 net.cpp:110] Creating Layer conv2
I1006 11:29:07.784317 26389 net.cpp:430] conv2 <- pool1
I1006 11:29:07.784322 26389 net.cpp:388] conv2 -> conv2
I1006 11:29:07.784327 26389 net.cpp:140] Setting up conv2
I1006 11:29:07.785094 26389 net.cpp:147] Top shape: 100 32 16 16 (819200)
I1006 11:29:07.785105 26389 layer_factory.hpp:74] Creating layer relu2
I1006 11:29:07.785111 26389 net.cpp:110] Creating Layer relu2
I1006 11:29:07.785114 26389 net.cpp:430] relu2 <- conv2
I1006 11:29:07.785117 26389 net.cpp:377] relu2 -> conv2 (in-place)
I1006 11:29:07.785122 26389 net.cpp:140] Setting up relu2
I1006 11:29:07.785257 26389 net.cpp:147] Top shape: 100 32 16 16 (819200)
I1006 11:29:07.785264 26389 layer_factory.hpp:74] Creating layer pool2
I1006 11:29:07.785270 26389 net.cpp:110] Creating Layer pool2
I1006 11:29:07.785272 26389 net.cpp:430] pool2 <- conv2
I1006 11:29:07.785276 26389 net.cpp:388] pool2 -> pool2
I1006 11:29:07.785281 26389 net.cpp:140] Setting up pool2
I1006 11:29:07.785531 26389 net.cpp:147] Top shape: 100 32 8 8 (204800)
I1006 11:29:07.785537 26389 layer_factory.hpp:74] Creating layer conv3
I1006 11:29:07.785543 26389 net.cpp:110] Creating Layer conv3
I1006 11:29:07.785547 26389 net.cpp:430] conv3 <- pool2
I1006 11:29:07.785552 26389 net.cpp:388] conv3 -> conv3
I1006 11:29:07.785557 26389 net.cpp:140] Setting up conv3
I1006 11:29:07.786533 26389 net.cpp:147] Top shape: 100 64 8 8 (409600)
I1006 11:29:07.786545 26389 layer_factory.hpp:74] Creating layer relu3
I1006 11:29:07.786552 26389 net.cpp:110] Creating Layer relu3
I1006 11:29:07.786556 26389 net.cpp:430] relu3 <- conv3
I1006 11:29:07.786559 26389 net.cpp:377] relu3 -> conv3 (in-place)
I1006 11:29:07.786563 26389 net.cpp:140] Setting up relu3
I1006 11:29:07.786700 26389 net.cpp:147] Top shape: 100 64 8 8 (409600)
I1006 11:29:07.786705 26389 layer_factory.hpp:74] Creating layer pool3
I1006 11:29:07.786710 26389 net.cpp:110] Creating Layer pool3
I1006 11:29:07.786713 26389 net.cpp:430] pool3 <- conv3
I1006 11:29:07.786717 26389 net.cpp:388] pool3 -> pool3
I1006 11:29:07.786722 26389 net.cpp:140] Setting up pool3
I1006 11:29:07.786936 26389 net.cpp:147] Top shape: 100 64 4 4 (102400)
I1006 11:29:07.786943 26389 layer_factory.hpp:74] Creating layer ip1
I1006 11:29:07.786952 26389 net.cpp:110] Creating Layer ip1
I1006 11:29:07.786954 26389 net.cpp:430] ip1 <- pool3
I1006 11:29:07.786958 26389 net.cpp:388] ip1 -> ip1
I1006 11:29:07.786967 26389 net.cpp:140] Setting up ip1
I1006 11:29:07.787576 26389 net.cpp:147] Top shape: 100 64 (6400)
I1006 11:29:07.787585 26389 layer_factory.hpp:74] Creating layer ip2
I1006 11:29:07.787591 26389 net.cpp:110] Creating Layer ip2
I1006 11:29:07.787595 26389 net.cpp:430] ip2 <- ip1
I1006 11:29:07.787598 26389 net.cpp:388] ip2 -> ip2
I1006 11:29:07.787603 26389 net.cpp:140] Setting up ip2
I1006 11:29:07.787617 26389 net.cpp:147] Top shape: 100 10 (1000)
I1006 11:29:07.787636 26389 layer_factory.hpp:74] Creating layer loss
I1006 11:29:07.787642 26389 net.cpp:110] Creating Layer loss
I1006 11:29:07.787645 26389 net.cpp:430] loss <- ip2
I1006 11:29:07.787648 26389 net.cpp:430] loss <- label
I1006 11:29:07.787655 26389 net.cpp:388] loss -> loss
I1006 11:29:07.787660 26389 net.cpp:140] Setting up loss
I1006 11:29:07.787665 26389 layer_factory.hpp:74] Creating layer loss
I1006 11:29:07.787802 26389 net.cpp:147] Top shape: (1)
I1006 11:29:07.787807 26389 net.cpp:149]     with loss weight 1
I1006 11:29:07.787824 26389 net.cpp:212] loss needs backward computation.
I1006 11:29:07.787829 26389 net.cpp:212] ip2 needs backward computation.
I1006 11:29:07.787832 26389 net.cpp:212] ip1 needs backward computation.
I1006 11:29:07.787834 26389 net.cpp:212] pool3 needs backward computation.
I1006 11:29:07.787837 26389 net.cpp:212] relu3 needs backward computation.
I1006 11:29:07.787839 26389 net.cpp:212] conv3 needs backward computation.
I1006 11:29:07.787842 26389 net.cpp:212] pool2 needs backward computation.
I1006 11:29:07.787843 26389 net.cpp:212] relu2 needs backward computation.
I1006 11:29:07.787847 26389 net.cpp:212] conv2 needs backward computation.
I1006 11:29:07.787848 26389 net.cpp:212] relu1 needs backward computation.
I1006 11:29:07.787863 26389 net.cpp:212] pool1 needs backward computation.
I1006 11:29:07.787864 26389 net.cpp:212] conv1 needs backward computation.
I1006 11:29:07.787868 26389 net.cpp:214] cifar does not need backward computation.
I1006 11:29:07.787870 26389 net.cpp:255] This network produces output loss
I1006 11:29:07.787880 26389 net.cpp:267] Network initialization done.
I1006 11:29:07.787884 26389 net.cpp:268] Memory required for data: 31978804
I1006 11:29:07.788177 26389 solver.cpp:164] Creating test net (#0) specified by net file: models/cifar10_noisy_gt_ft_clean_trainval.prototxt
I1006 11:29:07.788199 26389 net.cpp:307] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1006 11:29:07.788372 26389 net.cpp:46] Initializing net from parameters: 
name: "cifar10_noisy_gt"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "external/exp/db/cifar10/cifar10_mean.binaryproto"
  }
  data_param {
    source: "external/exp/db/cifar10/test"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1006 11:29:07.788431 26389 layer_factory.hpp:74] Creating layer cifar
I1006 11:29:07.788439 26389 net.cpp:110] Creating Layer cifar
I1006 11:29:07.788444 26389 net.cpp:388] cifar -> data
I1006 11:29:07.788450 26389 net.cpp:388] cifar -> label
I1006 11:29:07.788455 26389 net.cpp:140] Setting up cifar
I1006 11:29:07.788458 26389 data_transformer.cpp:22] Loading mean file from: external/exp/db/cifar10/cifar10_mean.binaryproto
I1006 11:29:07.788537 26389 db_lmdb.cpp:22] Opened lmdb external/exp/db/cifar10/test
I1006 11:29:07.788554 26389 data_layer.cpp:55] Skipping first 0 data points.
I1006 11:29:07.788566 26389 data_layer.cpp:100] output data size: 100,3,32,32
I1006 11:29:07.788607 26389 net.cpp:147] Top shape: 100 3 32 32 (307200)
I1006 11:29:07.788614 26389 net.cpp:147] Top shape: 100 (100)
I1006 11:29:07.788617 26389 layer_factory.hpp:74] Creating layer label_cifar_1_split
I1006 11:29:07.788624 26389 net.cpp:110] Creating Layer label_cifar_1_split
I1006 11:29:07.788638 26389 net.cpp:430] label_cifar_1_split <- label
I1006 11:29:07.788645 26389 net.cpp:388] label_cifar_1_split -> label_cifar_1_split_0
I1006 11:29:07.788650 26389 net.cpp:388] label_cifar_1_split -> label_cifar_1_split_1
I1006 11:29:07.788666 26389 net.cpp:140] Setting up label_cifar_1_split
I1006 11:29:07.788682 26389 net.cpp:147] Top shape: 100 (100)
I1006 11:29:07.788686 26389 net.cpp:147] Top shape: 100 (100)
I1006 11:29:07.788691 26389 layer_factory.hpp:74] Creating layer conv1
I1006 11:29:07.788697 26389 net.cpp:110] Creating Layer conv1
I1006 11:29:07.788699 26389 net.cpp:430] conv1 <- data
I1006 11:29:07.788704 26389 net.cpp:388] conv1 -> conv1
I1006 11:29:07.788710 26389 net.cpp:140] Setting up conv1
I1006 11:29:07.789293 26389 net.cpp:147] Top shape: 100 32 32 32 (3276800)
I1006 11:29:07.789304 26389 layer_factory.hpp:74] Creating layer pool1
I1006 11:29:07.789309 26389 net.cpp:110] Creating Layer pool1
I1006 11:29:07.789312 26389 net.cpp:430] pool1 <- conv1
I1006 11:29:07.789319 26389 net.cpp:388] pool1 -> pool1
I1006 11:29:07.789324 26389 net.cpp:140] Setting up pool1
I1006 11:29:07.789541 26389 net.cpp:147] Top shape: 100 32 16 16 (819200)
I1006 11:29:07.789548 26389 layer_factory.hpp:74] Creating layer relu1
I1006 11:29:07.789554 26389 net.cpp:110] Creating Layer relu1
I1006 11:29:07.789566 26389 net.cpp:430] relu1 <- pool1
I1006 11:29:07.789572 26389 net.cpp:377] relu1 -> pool1 (in-place)
I1006 11:29:07.789577 26389 net.cpp:140] Setting up relu1
I1006 11:29:07.789717 26389 net.cpp:147] Top shape: 100 32 16 16 (819200)
I1006 11:29:07.789724 26389 layer_factory.hpp:74] Creating layer conv2
I1006 11:29:07.789729 26389 net.cpp:110] Creating Layer conv2
I1006 11:29:07.789732 26389 net.cpp:430] conv2 <- pool1
I1006 11:29:07.789739 26389 net.cpp:388] conv2 -> conv2
I1006 11:29:07.789748 26389 net.cpp:140] Setting up conv2
I1006 11:29:07.790513 26389 net.cpp:147] Top shape: 100 32 16 16 (819200)
I1006 11:29:07.790524 26389 layer_factory.hpp:74] Creating layer relu2
I1006 11:29:07.790529 26389 net.cpp:110] Creating Layer relu2
I1006 11:29:07.790532 26389 net.cpp:430] relu2 <- conv2
I1006 11:29:07.790536 26389 net.cpp:377] relu2 -> conv2 (in-place)
I1006 11:29:07.790540 26389 net.cpp:140] Setting up relu2
I1006 11:29:07.790655 26389 net.cpp:147] Top shape: 100 32 16 16 (819200)
I1006 11:29:07.790664 26389 layer_factory.hpp:74] Creating layer pool2
I1006 11:29:07.790669 26389 net.cpp:110] Creating Layer pool2
I1006 11:29:07.790671 26389 net.cpp:430] pool2 <- conv2
I1006 11:29:07.790676 26389 net.cpp:388] pool2 -> pool2
I1006 11:29:07.790681 26389 net.cpp:140] Setting up pool2
I1006 11:29:07.790874 26389 net.cpp:147] Top shape: 100 32 8 8 (204800)
I1006 11:29:07.790880 26389 layer_factory.hpp:74] Creating layer conv3
I1006 11:29:07.790886 26389 net.cpp:110] Creating Layer conv3
I1006 11:29:07.790889 26389 net.cpp:430] conv3 <- pool2
I1006 11:29:07.790895 26389 net.cpp:388] conv3 -> conv3
I1006 11:29:07.790901 26389 net.cpp:140] Setting up conv3
I1006 11:29:07.791919 26389 net.cpp:147] Top shape: 100 64 8 8 (409600)
I1006 11:29:07.791932 26389 layer_factory.hpp:74] Creating layer relu3
I1006 11:29:07.791937 26389 net.cpp:110] Creating Layer relu3
I1006 11:29:07.791940 26389 net.cpp:430] relu3 <- conv3
I1006 11:29:07.791945 26389 net.cpp:377] relu3 -> conv3 (in-place)
I1006 11:29:07.791950 26389 net.cpp:140] Setting up relu3
I1006 11:29:07.792075 26389 net.cpp:147] Top shape: 100 64 8 8 (409600)
I1006 11:29:07.792081 26389 layer_factory.hpp:74] Creating layer pool3
I1006 11:29:07.792085 26389 net.cpp:110] Creating Layer pool3
I1006 11:29:07.792088 26389 net.cpp:430] pool3 <- conv3
I1006 11:29:07.792093 26389 net.cpp:388] pool3 -> pool3
I1006 11:29:07.792098 26389 net.cpp:140] Setting up pool3
I1006 11:29:07.792290 26389 net.cpp:147] Top shape: 100 64 4 4 (102400)
I1006 11:29:07.792299 26389 layer_factory.hpp:74] Creating layer ip1
I1006 11:29:07.792304 26389 net.cpp:110] Creating Layer ip1
I1006 11:29:07.792306 26389 net.cpp:430] ip1 <- pool3
I1006 11:29:07.792312 26389 net.cpp:388] ip1 -> ip1
I1006 11:29:07.792317 26389 net.cpp:140] Setting up ip1
I1006 11:29:07.792915 26389 net.cpp:147] Top shape: 100 64 (6400)
I1006 11:29:07.792923 26389 layer_factory.hpp:74] Creating layer ip2
I1006 11:29:07.792928 26389 net.cpp:110] Creating Layer ip2
I1006 11:29:07.792932 26389 net.cpp:430] ip2 <- ip1
I1006 11:29:07.792937 26389 net.cpp:388] ip2 -> ip2
I1006 11:29:07.792943 26389 net.cpp:140] Setting up ip2
I1006 11:29:07.792958 26389 net.cpp:147] Top shape: 100 10 (1000)
I1006 11:29:07.792965 26389 layer_factory.hpp:74] Creating layer ip2_ip2_0_split
I1006 11:29:07.792970 26389 net.cpp:110] Creating Layer ip2_ip2_0_split
I1006 11:29:07.792973 26389 net.cpp:430] ip2_ip2_0_split <- ip2
I1006 11:29:07.792976 26389 net.cpp:388] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1006 11:29:07.792981 26389 net.cpp:388] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1006 11:29:07.792986 26389 net.cpp:140] Setting up ip2_ip2_0_split
I1006 11:29:07.792991 26389 net.cpp:147] Top shape: 100 10 (1000)
I1006 11:29:07.792994 26389 net.cpp:147] Top shape: 100 10 (1000)
I1006 11:29:07.792997 26389 layer_factory.hpp:74] Creating layer accuracy
I1006 11:29:07.793002 26389 net.cpp:110] Creating Layer accuracy
I1006 11:29:07.793004 26389 net.cpp:430] accuracy <- ip2_ip2_0_split_0
I1006 11:29:07.793009 26389 net.cpp:430] accuracy <- label_cifar_1_split_0
I1006 11:29:07.793012 26389 net.cpp:388] accuracy -> accuracy
I1006 11:29:07.793017 26389 net.cpp:140] Setting up accuracy
I1006 11:29:07.793022 26389 net.cpp:147] Top shape: (1)
I1006 11:29:07.793025 26389 layer_factory.hpp:74] Creating layer loss
I1006 11:29:07.793028 26389 net.cpp:110] Creating Layer loss
I1006 11:29:07.793031 26389 net.cpp:430] loss <- ip2_ip2_0_split_1
I1006 11:29:07.793035 26389 net.cpp:430] loss <- label_cifar_1_split_1
I1006 11:29:07.793038 26389 net.cpp:388] loss -> loss
I1006 11:29:07.793042 26389 net.cpp:140] Setting up loss
I1006 11:29:07.793046 26389 layer_factory.hpp:74] Creating layer loss
I1006 11:29:07.793248 26389 net.cpp:147] Top shape: (1)
I1006 11:29:07.793254 26389 net.cpp:149]     with loss weight 1
I1006 11:29:07.793261 26389 net.cpp:212] loss needs backward computation.
I1006 11:29:07.793264 26389 net.cpp:214] accuracy does not need backward computation.
I1006 11:29:07.793267 26389 net.cpp:212] ip2_ip2_0_split needs backward computation.
I1006 11:29:07.793269 26389 net.cpp:212] ip2 needs backward computation.
I1006 11:29:07.793272 26389 net.cpp:212] ip1 needs backward computation.
I1006 11:29:07.793275 26389 net.cpp:212] pool3 needs backward computation.
I1006 11:29:07.793277 26389 net.cpp:212] relu3 needs backward computation.
I1006 11:29:07.793280 26389 net.cpp:212] conv3 needs backward computation.
I1006 11:29:07.793282 26389 net.cpp:212] pool2 needs backward computation.
I1006 11:29:07.793285 26389 net.cpp:212] relu2 needs backward computation.
I1006 11:29:07.793287 26389 net.cpp:212] conv2 needs backward computation.
I1006 11:29:07.793290 26389 net.cpp:212] relu1 needs backward computation.
I1006 11:29:07.793292 26389 net.cpp:212] pool1 needs backward computation.
I1006 11:29:07.793294 26389 net.cpp:212] conv1 needs backward computation.
I1006 11:29:07.793298 26389 net.cpp:214] label_cifar_1_split does not need backward computation.
I1006 11:29:07.793300 26389 net.cpp:214] cifar does not need backward computation.
I1006 11:29:07.793303 26389 net.cpp:255] This network produces output accuracy
I1006 11:29:07.793306 26389 net.cpp:255] This network produces output loss
I1006 11:29:07.793316 26389 net.cpp:267] Network initialization done.
I1006 11:29:07.793319 26389 net.cpp:268] Memory required for data: 31987608
I1006 11:29:07.793351 26389 solver.cpp:47] Solver scaffolding done.
I1006 11:29:07.793371 26389 caffe.cpp:110] Finetuning from external/exp/models/cifar10_clean_iter_5000.caffemodel
I1006 11:29:07.794042 26389 solver.cpp:250] Solving cifar10_noisy_gt
I1006 11:29:07.794049 26389 solver.cpp:251] Learning Rate Policy: step
I1006 11:29:07.794476 26389 solver.cpp:293] Iteration 0, Testing net (#0)
I1006 11:29:08.133332 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6297
I1006 11:29:08.133358 26389 solver.cpp:342]     Test net output #1: loss = 2.23482 (* 1 = 2.23482 loss)
I1006 11:29:08.138366 26389 solver.cpp:212] Iteration 0, loss = 5.01602
I1006 11:29:08.138383 26389 solver.cpp:229]     Train net output #0: loss = 5.01602 (* 1 = 5.01602 loss)
I1006 11:29:08.138396 26389 solver.cpp:507] Iteration 0, lr = 0.001
I1006 11:29:09.091271 26389 solver.cpp:212] Iteration 100, loss = 2.11072
I1006 11:29:09.091312 26389 solver.cpp:229]     Train net output #0: loss = 1.80075 (* 1 = 1.80075 loss)
I1006 11:29:09.091318 26389 solver.cpp:507] Iteration 100, lr = 0.001
I1006 11:29:10.050436 26389 solver.cpp:212] Iteration 200, loss = 1.78578
I1006 11:29:10.050474 26389 solver.cpp:229]     Train net output #0: loss = 1.75655 (* 1 = 1.75655 loss)
I1006 11:29:10.050482 26389 solver.cpp:507] Iteration 200, lr = 0.001
I1006 11:29:11.008883 26389 solver.cpp:212] Iteration 300, loss = 1.73368
I1006 11:29:11.008910 26389 solver.cpp:229]     Train net output #0: loss = 1.95146 (* 1 = 1.95146 loss)
I1006 11:29:11.008916 26389 solver.cpp:507] Iteration 300, lr = 0.001
I1006 11:29:11.966579 26389 solver.cpp:212] Iteration 400, loss = 1.74396
I1006 11:29:11.966606 26389 solver.cpp:229]     Train net output #0: loss = 1.76546 (* 1 = 1.76546 loss)
I1006 11:29:11.966612 26389 solver.cpp:507] Iteration 400, lr = 0.001
I1006 11:29:12.915478 26389 solver.cpp:293] Iteration 500, Testing net (#0)
I1006 11:29:13.229158 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6219
I1006 11:29:13.229187 26389 solver.cpp:342]     Test net output #1: loss = 1.25144 (* 1 = 1.25144 loss)
I1006 11:29:13.232532 26389 solver.cpp:212] Iteration 500, loss = 1.70848
I1006 11:29:13.232561 26389 solver.cpp:229]     Train net output #0: loss = 1.4646 (* 1 = 1.4646 loss)
I1006 11:29:13.232568 26389 solver.cpp:507] Iteration 500, lr = 0.001
I1006 11:29:14.189818 26389 solver.cpp:212] Iteration 600, loss = 1.62838
I1006 11:29:14.189846 26389 solver.cpp:229]     Train net output #0: loss = 1.70244 (* 1 = 1.70244 loss)
I1006 11:29:14.189852 26389 solver.cpp:507] Iteration 600, lr = 0.001
I1006 11:29:15.147939 26389 solver.cpp:212] Iteration 700, loss = 1.62871
I1006 11:29:15.147966 26389 solver.cpp:229]     Train net output #0: loss = 1.64146 (* 1 = 1.64146 loss)
I1006 11:29:15.147972 26389 solver.cpp:507] Iteration 700, lr = 0.001
I1006 11:29:16.105859 26389 solver.cpp:212] Iteration 800, loss = 1.60948
I1006 11:29:16.105885 26389 solver.cpp:229]     Train net output #0: loss = 1.83683 (* 1 = 1.83683 loss)
I1006 11:29:16.105890 26389 solver.cpp:507] Iteration 800, lr = 0.001
I1006 11:29:17.064075 26389 solver.cpp:212] Iteration 900, loss = 1.64669
I1006 11:29:17.064101 26389 solver.cpp:229]     Train net output #0: loss = 1.67724 (* 1 = 1.67724 loss)
I1006 11:29:17.064107 26389 solver.cpp:507] Iteration 900, lr = 0.001
I1006 11:29:18.013788 26389 solver.cpp:293] Iteration 1000, Testing net (#0)
I1006 11:29:18.328387 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6436
I1006 11:29:18.328415 26389 solver.cpp:342]     Test net output #1: loss = 1.18751 (* 1 = 1.18751 loss)
I1006 11:29:18.331760 26389 solver.cpp:212] Iteration 1000, loss = 1.61865
I1006 11:29:18.331778 26389 solver.cpp:229]     Train net output #0: loss = 1.39165 (* 1 = 1.39165 loss)
I1006 11:29:18.331784 26389 solver.cpp:507] Iteration 1000, lr = 0.001
I1006 11:29:19.292035 26389 solver.cpp:212] Iteration 1100, loss = 1.55685
I1006 11:29:19.292075 26389 solver.cpp:229]     Train net output #0: loss = 1.60128 (* 1 = 1.60128 loss)
I1006 11:29:19.292083 26389 solver.cpp:507] Iteration 1100, lr = 0.001
I1006 11:29:20.250602 26389 solver.cpp:212] Iteration 1200, loss = 1.56586
I1006 11:29:20.250641 26389 solver.cpp:229]     Train net output #0: loss = 1.56967 (* 1 = 1.56967 loss)
I1006 11:29:20.250648 26389 solver.cpp:507] Iteration 1200, lr = 0.001
I1006 11:29:21.209746 26389 solver.cpp:212] Iteration 1300, loss = 1.55101
I1006 11:29:21.209786 26389 solver.cpp:229]     Train net output #0: loss = 1.77882 (* 1 = 1.77882 loss)
I1006 11:29:21.209794 26389 solver.cpp:507] Iteration 1300, lr = 0.001
I1006 11:29:22.168316 26389 solver.cpp:212] Iteration 1400, loss = 1.59482
I1006 11:29:22.168344 26389 solver.cpp:229]     Train net output #0: loss = 1.63136 (* 1 = 1.63136 loss)
I1006 11:29:22.168349 26389 solver.cpp:507] Iteration 1400, lr = 0.001
I1006 11:29:23.118254 26389 solver.cpp:293] Iteration 1500, Testing net (#0)
I1006 11:29:23.432433 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6513
I1006 11:29:23.432461 26389 solver.cpp:342]     Test net output #1: loss = 1.15779 (* 1 = 1.15779 loss)
I1006 11:29:23.435843 26389 solver.cpp:212] Iteration 1500, loss = 1.56915
I1006 11:29:23.435873 26389 solver.cpp:229]     Train net output #0: loss = 1.35824 (* 1 = 1.35824 loss)
I1006 11:29:23.435880 26389 solver.cpp:507] Iteration 1500, lr = 0.001
I1006 11:29:24.397037 26389 solver.cpp:212] Iteration 1600, loss = 1.513
I1006 11:29:24.397064 26389 solver.cpp:229]     Train net output #0: loss = 1.54816 (* 1 = 1.54816 loss)
I1006 11:29:24.397070 26389 solver.cpp:507] Iteration 1600, lr = 0.001
I1006 11:29:25.355038 26389 solver.cpp:212] Iteration 1700, loss = 1.52203
I1006 11:29:25.355078 26389 solver.cpp:229]     Train net output #0: loss = 1.54951 (* 1 = 1.54951 loss)
I1006 11:29:25.355087 26389 solver.cpp:507] Iteration 1700, lr = 0.001
I1006 11:29:26.313814 26389 solver.cpp:212] Iteration 1800, loss = 1.51136
I1006 11:29:26.313840 26389 solver.cpp:229]     Train net output #0: loss = 1.70674 (* 1 = 1.70674 loss)
I1006 11:29:26.313846 26389 solver.cpp:507] Iteration 1800, lr = 0.001
I1006 11:29:27.272830 26389 solver.cpp:212] Iteration 1900, loss = 1.56058
I1006 11:29:27.272868 26389 solver.cpp:229]     Train net output #0: loss = 1.58893 (* 1 = 1.58893 loss)
I1006 11:29:27.272876 26389 solver.cpp:507] Iteration 1900, lr = 0.001
I1006 11:29:28.226035 26389 solver.cpp:293] Iteration 2000, Testing net (#0)
I1006 11:29:28.539275 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6561
I1006 11:29:28.539304 26389 solver.cpp:342]     Test net output #1: loss = 1.14399 (* 1 = 1.14399 loss)
I1006 11:29:28.542672 26389 solver.cpp:212] Iteration 2000, loss = 1.53334
I1006 11:29:28.542690 26389 solver.cpp:229]     Train net output #0: loss = 1.34796 (* 1 = 1.34796 loss)
I1006 11:29:28.542696 26389 solver.cpp:507] Iteration 2000, lr = 0.001
I1006 11:29:29.507206 26389 solver.cpp:212] Iteration 2100, loss = 1.48204
I1006 11:29:29.507247 26389 solver.cpp:229]     Train net output #0: loss = 1.49962 (* 1 = 1.49962 loss)
I1006 11:29:29.507253 26389 solver.cpp:507] Iteration 2100, lr = 0.001
I1006 11:29:30.471643 26389 solver.cpp:212] Iteration 2200, loss = 1.48551
I1006 11:29:30.471670 26389 solver.cpp:229]     Train net output #0: loss = 1.53732 (* 1 = 1.53732 loss)
I1006 11:29:30.471676 26389 solver.cpp:507] Iteration 2200, lr = 0.001
I1006 11:29:31.435721 26389 solver.cpp:212] Iteration 2300, loss = 1.47974
I1006 11:29:31.435760 26389 solver.cpp:229]     Train net output #0: loss = 1.65378 (* 1 = 1.65378 loss)
I1006 11:29:31.435766 26389 solver.cpp:507] Iteration 2300, lr = 0.001
I1006 11:29:32.403398 26389 solver.cpp:212] Iteration 2400, loss = 1.53358
I1006 11:29:32.403424 26389 solver.cpp:229]     Train net output #0: loss = 1.56124 (* 1 = 1.56124 loss)
I1006 11:29:32.403430 26389 solver.cpp:507] Iteration 2400, lr = 0.001
I1006 11:29:33.360002 26389 solver.cpp:293] Iteration 2500, Testing net (#0)
I1006 11:29:33.675153 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6577
I1006 11:29:33.675181 26389 solver.cpp:342]     Test net output #1: loss = 1.13196 (* 1 = 1.13196 loss)
I1006 11:29:33.678575 26389 solver.cpp:212] Iteration 2500, loss = 1.50239
I1006 11:29:33.678592 26389 solver.cpp:229]     Train net output #0: loss = 1.33185 (* 1 = 1.33185 loss)
I1006 11:29:33.678599 26389 solver.cpp:507] Iteration 2500, lr = 0.001
I1006 11:29:34.646378 26389 solver.cpp:212] Iteration 2600, loss = 1.45516
I1006 11:29:34.646405 26389 solver.cpp:229]     Train net output #0: loss = 1.47084 (* 1 = 1.47084 loss)
I1006 11:29:34.646411 26389 solver.cpp:507] Iteration 2600, lr = 0.001
I1006 11:29:35.614285 26389 solver.cpp:212] Iteration 2700, loss = 1.45546
I1006 11:29:35.614313 26389 solver.cpp:229]     Train net output #0: loss = 1.51249 (* 1 = 1.51249 loss)
I1006 11:29:35.614320 26389 solver.cpp:507] Iteration 2700, lr = 0.001
I1006 11:29:36.581374 26389 solver.cpp:212] Iteration 2800, loss = 1.45306
I1006 11:29:36.581401 26389 solver.cpp:229]     Train net output #0: loss = 1.62129 (* 1 = 1.62129 loss)
I1006 11:29:36.581408 26389 solver.cpp:507] Iteration 2800, lr = 0.001
I1006 11:29:37.548691 26389 solver.cpp:212] Iteration 2900, loss = 1.50992
I1006 11:29:37.548718 26389 solver.cpp:229]     Train net output #0: loss = 1.52545 (* 1 = 1.52545 loss)
I1006 11:29:37.548723 26389 solver.cpp:507] Iteration 2900, lr = 0.001
I1006 11:29:38.506441 26389 solver.cpp:293] Iteration 3000, Testing net (#0)
I1006 11:29:38.821557 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6568
I1006 11:29:38.821595 26389 solver.cpp:342]     Test net output #1: loss = 1.12647 (* 1 = 1.12647 loss)
I1006 11:29:38.825054 26389 solver.cpp:212] Iteration 3000, loss = 1.47944
I1006 11:29:38.825072 26389 solver.cpp:229]     Train net output #0: loss = 1.31375 (* 1 = 1.31375 loss)
I1006 11:29:38.825078 26389 solver.cpp:507] Iteration 3000, lr = 0.001
I1006 11:29:39.791642 26389 solver.cpp:212] Iteration 3100, loss = 1.43115
I1006 11:29:39.791671 26389 solver.cpp:229]     Train net output #0: loss = 1.43487 (* 1 = 1.43487 loss)
I1006 11:29:39.791676 26389 solver.cpp:507] Iteration 3100, lr = 0.001
I1006 11:29:40.759028 26389 solver.cpp:212] Iteration 3200, loss = 1.42843
I1006 11:29:40.759068 26389 solver.cpp:229]     Train net output #0: loss = 1.48945 (* 1 = 1.48945 loss)
I1006 11:29:40.759075 26389 solver.cpp:507] Iteration 3200, lr = 0.001
I1006 11:29:41.725637 26389 solver.cpp:212] Iteration 3300, loss = 1.42923
I1006 11:29:41.725663 26389 solver.cpp:229]     Train net output #0: loss = 1.60197 (* 1 = 1.60197 loss)
I1006 11:29:41.725669 26389 solver.cpp:507] Iteration 3300, lr = 0.001
I1006 11:29:42.692836 26389 solver.cpp:212] Iteration 3400, loss = 1.4895
I1006 11:29:42.692876 26389 solver.cpp:229]     Train net output #0: loss = 1.50767 (* 1 = 1.50767 loss)
I1006 11:29:42.692883 26389 solver.cpp:507] Iteration 3400, lr = 0.001
I1006 11:29:43.650852 26389 solver.cpp:293] Iteration 3500, Testing net (#0)
I1006 11:29:43.965803 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6562
I1006 11:29:43.965831 26389 solver.cpp:342]     Test net output #1: loss = 1.12212 (* 1 = 1.12212 loss)
I1006 11:29:43.969167 26389 solver.cpp:212] Iteration 3500, loss = 1.45533
I1006 11:29:43.969185 26389 solver.cpp:229]     Train net output #0: loss = 1.29034 (* 1 = 1.29034 loss)
I1006 11:29:43.969192 26389 solver.cpp:507] Iteration 3500, lr = 0.001
I1006 11:29:44.936452 26389 solver.cpp:212] Iteration 3600, loss = 1.4109
I1006 11:29:44.936480 26389 solver.cpp:229]     Train net output #0: loss = 1.4098 (* 1 = 1.4098 loss)
I1006 11:29:44.936486 26389 solver.cpp:507] Iteration 3600, lr = 0.001
I1006 11:29:45.903233 26389 solver.cpp:212] Iteration 3700, loss = 1.40274
I1006 11:29:45.903272 26389 solver.cpp:229]     Train net output #0: loss = 1.44905 (* 1 = 1.44905 loss)
I1006 11:29:45.903280 26389 solver.cpp:507] Iteration 3700, lr = 0.001
I1006 11:29:46.870017 26389 solver.cpp:212] Iteration 3800, loss = 1.40741
I1006 11:29:46.870056 26389 solver.cpp:229]     Train net output #0: loss = 1.5792 (* 1 = 1.5792 loss)
I1006 11:29:46.870062 26389 solver.cpp:507] Iteration 3800, lr = 0.001
I1006 11:29:47.837101 26389 solver.cpp:212] Iteration 3900, loss = 1.46923
I1006 11:29:47.837127 26389 solver.cpp:229]     Train net output #0: loss = 1.47942 (* 1 = 1.47942 loss)
I1006 11:29:47.837133 26389 solver.cpp:507] Iteration 3900, lr = 0.001
I1006 11:29:48.794914 26389 solver.cpp:293] Iteration 4000, Testing net (#0)
I1006 11:29:49.110014 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6543
I1006 11:29:49.110055 26389 solver.cpp:342]     Test net output #1: loss = 1.12628 (* 1 = 1.12628 loss)
I1006 11:29:49.113456 26389 solver.cpp:212] Iteration 4000, loss = 1.43594
I1006 11:29:49.113486 26389 solver.cpp:229]     Train net output #0: loss = 1.27235 (* 1 = 1.27235 loss)
I1006 11:29:49.113492 26389 solver.cpp:507] Iteration 4000, lr = 0.0001
I1006 11:29:50.085569 26389 solver.cpp:212] Iteration 4100, loss = 1.40793
I1006 11:29:50.085610 26389 solver.cpp:229]     Train net output #0: loss = 1.43813 (* 1 = 1.43813 loss)
I1006 11:29:50.085616 26389 solver.cpp:507] Iteration 4100, lr = 0.0001
I1006 11:29:51.103539 26389 solver.cpp:212] Iteration 4200, loss = 1.37104
I1006 11:29:51.103580 26389 solver.cpp:229]     Train net output #0: loss = 1.50426 (* 1 = 1.50426 loss)
I1006 11:29:51.103586 26389 solver.cpp:507] Iteration 4200, lr = 0.0001
I1006 11:29:52.137639 26389 solver.cpp:212] Iteration 4300, loss = 1.35038
I1006 11:29:52.137665 26389 solver.cpp:229]     Train net output #0: loss = 1.51539 (* 1 = 1.51539 loss)
I1006 11:29:52.137671 26389 solver.cpp:507] Iteration 4300, lr = 0.0001
I1006 11:29:53.182202 26389 solver.cpp:212] Iteration 4400, loss = 1.37885
I1006 11:29:53.182243 26389 solver.cpp:229]     Train net output #0: loss = 1.35046 (* 1 = 1.35046 loss)
I1006 11:29:53.182250 26389 solver.cpp:507] Iteration 4400, lr = 0.0001
I1006 11:29:54.222687 26389 solver.cpp:293] Iteration 4500, Testing net (#0)
I1006 11:29:54.563849 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6713
I1006 11:29:54.563889 26389 solver.cpp:342]     Test net output #1: loss = 1.09532 (* 1 = 1.09532 loss)
I1006 11:29:54.567531 26389 solver.cpp:212] Iteration 4500, loss = 1.31633
I1006 11:29:54.567549 26389 solver.cpp:229]     Train net output #0: loss = 1.2274 (* 1 = 1.2274 loss)
I1006 11:29:54.567556 26389 solver.cpp:507] Iteration 4500, lr = 0.0001
I1006 11:29:55.642655 26389 solver.cpp:212] Iteration 4600, loss = 1.35721
I1006 11:29:55.642681 26389 solver.cpp:229]     Train net output #0: loss = 1.39066 (* 1 = 1.39066 loss)
I1006 11:29:55.642688 26389 solver.cpp:507] Iteration 4600, lr = 0.0001
I1006 11:29:56.731560 26389 solver.cpp:212] Iteration 4700, loss = 1.34319
I1006 11:29:56.731611 26389 solver.cpp:229]     Train net output #0: loss = 1.47678 (* 1 = 1.47678 loss)
I1006 11:29:56.731617 26389 solver.cpp:507] Iteration 4700, lr = 0.0001
I1006 11:29:57.822371 26389 solver.cpp:212] Iteration 4800, loss = 1.33079
I1006 11:29:57.822396 26389 solver.cpp:229]     Train net output #0: loss = 1.49814 (* 1 = 1.49814 loss)
I1006 11:29:57.822402 26389 solver.cpp:507] Iteration 4800, lr = 0.0001
I1006 11:29:58.913074 26389 solver.cpp:212] Iteration 4900, loss = 1.36709
I1006 11:29:58.913115 26389 solver.cpp:229]     Train net output #0: loss = 1.33436 (* 1 = 1.33436 loss)
I1006 11:29:58.913121 26389 solver.cpp:507] Iteration 4900, lr = 0.0001
I1006 11:29:59.993072 26389 solver.cpp:293] Iteration 5000, Testing net (#0)
I1006 11:30:00.342012 26389 solver.cpp:342]     Test net output #0: accuracy = 0.67
I1006 11:30:00.342041 26389 solver.cpp:342]     Test net output #1: loss = 1.09648 (* 1 = 1.09648 loss)
I1006 11:30:00.345722 26389 solver.cpp:212] Iteration 5000, loss = 1.31426
I1006 11:30:00.345752 26389 solver.cpp:229]     Train net output #0: loss = 1.2171 (* 1 = 1.2171 loss)
I1006 11:30:00.345760 26389 solver.cpp:507] Iteration 5000, lr = 0.0001
I1006 11:30:01.436961 26389 solver.cpp:212] Iteration 5100, loss = 1.34367
I1006 11:30:01.437001 26389 solver.cpp:229]     Train net output #0: loss = 1.37685 (* 1 = 1.37685 loss)
I1006 11:30:01.437007 26389 solver.cpp:507] Iteration 5100, lr = 0.0001
I1006 11:30:02.538944 26389 solver.cpp:212] Iteration 5200, loss = 1.33061
I1006 11:30:02.538983 26389 solver.cpp:229]     Train net output #0: loss = 1.46343 (* 1 = 1.46343 loss)
I1006 11:30:02.538990 26389 solver.cpp:507] Iteration 5200, lr = 0.0001
I1006 11:30:03.646903 26389 solver.cpp:212] Iteration 5300, loss = 1.31999
I1006 11:30:03.646940 26389 solver.cpp:229]     Train net output #0: loss = 1.4849 (* 1 = 1.4849 loss)
I1006 11:30:03.646947 26389 solver.cpp:507] Iteration 5300, lr = 0.0001
I1006 11:30:04.753862 26389 solver.cpp:212] Iteration 5400, loss = 1.35999
I1006 11:30:04.753890 26389 solver.cpp:229]     Train net output #0: loss = 1.32928 (* 1 = 1.32928 loss)
I1006 11:30:04.753896 26389 solver.cpp:507] Iteration 5400, lr = 0.0001
I1006 11:30:05.850937 26389 solver.cpp:293] Iteration 5500, Testing net (#0)
I1006 11:30:06.203968 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6696
I1006 11:30:06.203994 26389 solver.cpp:342]     Test net output #1: loss = 1.09757 (* 1 = 1.09757 loss)
I1006 11:30:06.207731 26389 solver.cpp:212] Iteration 5500, loss = 1.31262
I1006 11:30:06.207761 26389 solver.cpp:229]     Train net output #0: loss = 1.20775 (* 1 = 1.20775 loss)
I1006 11:30:06.207768 26389 solver.cpp:507] Iteration 5500, lr = 0.0001
I1006 11:30:07.314090 26389 solver.cpp:212] Iteration 5600, loss = 1.33352
I1006 11:30:07.314129 26389 solver.cpp:229]     Train net output #0: loss = 1.36987 (* 1 = 1.36987 loss)
I1006 11:30:07.314136 26389 solver.cpp:507] Iteration 5600, lr = 0.0001
I1006 11:30:08.420107 26389 solver.cpp:212] Iteration 5700, loss = 1.32105
I1006 11:30:08.420146 26389 solver.cpp:229]     Train net output #0: loss = 1.45295 (* 1 = 1.45295 loss)
I1006 11:30:08.420153 26389 solver.cpp:507] Iteration 5700, lr = 0.0001
I1006 11:30:09.527169 26389 solver.cpp:212] Iteration 5800, loss = 1.31189
I1006 11:30:09.527210 26389 solver.cpp:229]     Train net output #0: loss = 1.47717 (* 1 = 1.47717 loss)
I1006 11:30:09.527216 26389 solver.cpp:507] Iteration 5800, lr = 0.0001
I1006 11:30:10.633829 26389 solver.cpp:212] Iteration 5900, loss = 1.35449
I1006 11:30:10.633867 26389 solver.cpp:229]     Train net output #0: loss = 1.32743 (* 1 = 1.32743 loss)
I1006 11:30:10.633873 26389 solver.cpp:507] Iteration 5900, lr = 0.0001
I1006 11:30:11.730252 26389 solver.cpp:379] Snapshotting to binary proto file external/exp/models/cifar10_noisy_gt_ft_clean_iter_6000.caffemodel
I1006 11:30:11.739382 26389 solver.cpp:689] Snapshotting solver state to binary proto fileexternal/exp/models/cifar10_noisy_gt_ft_clean_iter_6000.solverstate
I1006 11:30:11.743667 26389 solver.cpp:276] Iteration 6000, loss = 1.20028
I1006 11:30:11.743685 26389 solver.cpp:293] Iteration 6000, Testing net (#0)
I1006 11:30:12.089208 26389 solver.cpp:342]     Test net output #0: accuracy = 0.6685
I1006 11:30:12.089236 26389 solver.cpp:342]     Test net output #1: loss = 1.09936 (* 1 = 1.09936 loss)
I1006 11:30:12.089241 26389 solver.cpp:281] Optimization Done.
I1006 11:30:12.089244 26389 caffe.cpp:178] Optimization Done.
